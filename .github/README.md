# GitHub Actions - AI Compatibility Testing

Automated testing system to ensure consistent code quality across all major AI models.

## ğŸ“ Structure

```
.github/
â”œâ”€â”€ workflows/
â”‚   â””â”€â”€ ai-compatibility-tests.yml    # Main GitHub Actions workflow
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ test-ai.js                    # Test runner (calls AI APIs)
â”‚   â”œâ”€â”€ analyze-results.py            # Results analysis
â”‚   â”œâ”€â”€ generate-report.py            # Comparison reports
â”‚   â”œâ”€â”€ check-thresholds.py           # Quality validation
â”‚   â””â”€â”€ package.json                  # Node dependencies
â”œâ”€â”€ TESTING_GUIDE.md                  # Complete setup guide
â””â”€â”€ README.md                         # This file
```

## ğŸš€ Quick Start

### 1. Add API Keys

Go to **Settings â†’ Secrets â†’ New repository secret**:

- `OPENAI_API_KEY` - Required for GPT-4
- `ANTHROPIC_API_KEY` - Required for Claude
- `GOOGLE_API_KEY` - Optional for Gemini
- `MISTRAL_API_KEY` - Optional for Codestral

### 2. Run Tests

**Automatic**: Push to `main` or create a PR  
**Manual**: Actions â†’ AI Compatibility Tests â†’ Run workflow

### 3. View Results

Actions â†’ Latest run â†’ Download artifacts or view summary

## ğŸ“Š What Gets Tested

- âœ… **Spring Boot**: Constructor injection, DTOs, transactions
- âœ… **React**: Functional components, useEffect deps, hooks
- âœ… **ASP.NET Core**: DI patterns, DTOs, async/await
- âœ… **FastAPI**: Async patterns, Pydantic models
- âœ… **Next.js**: Server vs Client Components

## ğŸ“ˆ Success Criteria

Tests pass when:
- âœ… Average score â‰¥ 90/100
- âœ… Pass rate â‰¥ 90%
- âœ… All models within 10% of each other

## ğŸ“– Documentation

- **Full Guide**: [TESTING_GUIDE.md](TESTING_GUIDE.md)
- **Test Definitions**: [../.ai-iap/TEST_PROMPTS.md](../.ai-iap/TEST_PROMPTS.md)
- **Improvement Strategies**: [../PRIORITY_ACTIONS.md](../PRIORITY_ACTIONS.md)

## ğŸ”§ Local Testing

```bash
cd .github/scripts
npm install

export OPENAI_API_KEY="sk-..."
node test-ai.js --model gpt-4-turbo-preview --provider openai --test-suite critical
```

## ğŸ’¡ Troubleshooting

**Tests not running?**
- Check GitHub Actions are enabled
- Verify API keys are set correctly

**Tests failing?**
- Download artifacts to see AI outputs
- Compare against expected patterns
- Review TESTING_GUIDE.md

## ğŸ“ Need Help?

See [TESTING_GUIDE.md](TESTING_GUIDE.md) for comprehensive documentation.


